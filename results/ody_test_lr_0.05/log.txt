2018-12-06 15:46:57 - INFO - saving to ./results/ody_test_lr_0.05
2018-12-06 15:46:57 - DEBUG - run arguments: Namespace(batch_size=256, dataset='cifar10', device='cuda', device_ids=[1], dtype='float', epochs=250, evaluate=None, input_size=None, lr=0.05, model='resnet_quantized_float_bn_pruning', model_config="{'depth': 18}", momentum=0.9, optimizer='SGD', print_freq=40, results_dir='./results', resume='./results/quantized_resnet18_pruning/model_best.pth.tar', save='ody_test_lr_0.05', seed=123, start_epoch=0, weight_decay=0.0001, workers=16)
2018-12-06 15:46:57 - INFO - creating model resnet_quantized_float_bn_pruning
2018-12-06 15:46:57 - INFO - created model with configuration: {'input_size': None, 'dataset': 'cifar10', 'depth': 18}
2018-12-06 15:46:57 - INFO - loading checkpoint './results/quantized_resnet18_pruning/model_best.pth.tar'
2018-12-06 15:47:00 - INFO - loaded checkpoint './results/quantized_resnet18_pruning/model_best.pth.tar' (epoch 134)
2018-12-06 15:47:00 - INFO - number of parameters: 175258
2018-12-06 15:47:04 - INFO - EVALUATING - Epoch: [0][39/40]	Time 0.022 (0.075)	Data 0.000 (0.029)	Loss 0.1665 (0.3821)	Prec@1 87.500 (88.780)	Prec@5 100.000 (99.660)
2018-12-06 15:47:05 - INFO - training regime: [{'epoch': 0, 'optimizer': 'SGD', 'lr': 0.1, 'weight_decay': 0.0001, 'momentum': 0.9}, {'epoch': 81, 'lr': 0.01}, {'epoch': 122, 'lr': 0.001, 'weight_decay': 0}, {'epoch': 164, 'lr': 0.0001}]
2018-12-06 15:47:05 - INFO - Magnitude of weights: 0.09550750255584717
2018-12-06 15:47:05 - INFO - Magnitude of distance: 0.03503008186817169
2018-12-06 15:47:05 - INFO - STD of distance: 0.11103439331054688
2018-12-06 15:47:05 - INFO - 0-Sketch: 
2018-12-06 15:47:08 - INFO - EVALUATING - Epoch: [0][39/40]	Time 0.009 (0.074)	Data 0.000 (0.031)	Loss 6.8071 (6.3139)	Prec@1 12.500 (11.390)	Prec@5 62.500 (57.900)
2018-12-06 15:47:09 - INFO - Start Retraining ...
2018-12-06 15:47:11 - DEBUG - OPTIMIZER - setting lr = 0.1
2018-12-06 15:47:11 - DEBUG - OPTIMIZER - setting momentum = 0.9
2018-12-06 15:47:11 - DEBUG - OPTIMIZER - setting weight_decay = 0.0001
2018-12-06 15:47:27 - INFO - TRAINING - Epoch: [1][195/196]	Time 0.110 (0.095)	Data 0.000 (0.007)	Loss 0.3230 (0.3783)	Prec@1 88.750 (86.804)	Prec@5 100.000 (99.606)
2018-12-06 15:47:46 - INFO - TRAINING - Epoch: [2][195/196]	Time 0.026 (0.096)	Data 0.000 (0.009)	Loss 0.6609 (0.3404)	Prec@1 76.250 (88.020)	Prec@5 98.750 (99.722)
2018-12-06 15:48:05 - INFO - TRAINING - Epoch: [3][195/196]	Time 0.026 (0.095)	Data 0.000 (0.007)	Loss 0.2407 (0.3270)	Prec@1 92.500 (88.472)	Prec@5 100.000 (99.708)
2018-12-06 15:48:28 - INFO - TRAINING - Epoch: [4][195/196]	Time 0.043 (0.119)	Data 0.000 (0.008)	Loss 0.5117 (0.3155)	Prec@1 82.500 (88.940)	Prec@5 100.000 (99.718)
2018-12-06 15:49:01 - INFO - TRAINING - Epoch: [5][195/196]	Time 0.095 (0.163)	Data 0.000 (0.012)	Loss 0.3320 (0.3172)	Prec@1 87.500 (88.852)	Prec@5 100.000 (99.704)
2018-12-06 15:49:39 - INFO - TRAINING - Epoch: [6][195/196]	Time 0.071 (0.194)	Data 0.000 (0.021)	Loss 0.4192 (0.3131)	Prec@1 87.500 (88.952)	Prec@5 100.000 (99.742)
2018-12-06 15:50:17 - INFO - TRAINING - Epoch: [7][195/196]	Time 0.248 (0.192)	Data 0.000 (0.021)	Loss 0.2091 (0.3095)	Prec@1 91.250 (89.202)	Prec@5 100.000 (99.734)
2018-12-06 15:50:53 - INFO - TRAINING - Epoch: [8][195/196]	Time 0.070 (0.187)	Data 0.000 (0.021)	Loss 0.3163 (0.3069)	Prec@1 88.750 (89.160)	Prec@5 100.000 (99.738)
2018-12-06 15:51:32 - INFO - TRAINING - Epoch: [9][195/196]	Time 0.052 (0.193)	Data 0.000 (0.029)	Loss 0.1548 (0.3034)	Prec@1 95.000 (89.344)	Prec@5 100.000 (99.736)
2018-12-06 15:52:10 - INFO - TRAINING - Epoch: [10][195/196]	Time 0.035 (0.196)	Data 0.000 (0.028)	Loss 0.2947 (0.3056)	Prec@1 86.250 (89.280)	Prec@5 100.000 (99.756)
2018-12-06 15:52:49 - INFO - TRAINING - Epoch: [11][195/196]	Time 0.047 (0.195)	Data 0.000 (0.027)	Loss 0.3711 (0.3053)	Prec@1 90.000 (89.154)	Prec@5 100.000 (99.734)
2018-12-06 15:53:27 - INFO - TRAINING - Epoch: [12][195/196]	Time 0.040 (0.194)	Data 0.000 (0.026)	Loss 0.4318 (0.3034)	Prec@1 83.750 (89.546)	Prec@5 100.000 (99.748)
2018-12-06 15:54:06 - INFO - TRAINING - Epoch: [13][195/196]	Time 0.048 (0.196)	Data 0.000 (0.025)	Loss 0.1933 (0.3015)	Prec@1 93.750 (89.554)	Prec@5 100.000 (99.720)
2018-12-06 15:54:44 - INFO - TRAINING - Epoch: [14][195/196]	Time 0.047 (0.193)	Data 0.000 (0.024)	Loss 0.1694 (0.2952)	Prec@1 95.000 (89.632)	Prec@5 100.000 (99.742)
2018-12-06 15:55:22 - INFO - TRAINING - Epoch: [15][195/196]	Time 0.050 (0.197)	Data 0.000 (0.023)	Loss 0.4086 (0.2973)	Prec@1 86.250 (89.632)	Prec@5 98.750 (99.742)
2018-12-06 15:56:01 - INFO - TRAINING - Epoch: [16][195/196]	Time 0.039 (0.194)	Data 0.000 (0.022)	Loss 0.2232 (0.2910)	Prec@1 92.500 (89.704)	Prec@5 100.000 (99.772)
2018-12-06 15:56:39 - INFO - TRAINING - Epoch: [17][195/196]	Time 0.055 (0.195)	Data 0.000 (0.017)	Loss 0.3220 (0.2962)	Prec@1 87.500 (89.646)	Prec@5 100.000 (99.744)
2018-12-06 15:57:17 - INFO - TRAINING - Epoch: [18][195/196]	Time 0.067 (0.193)	Data 0.000 (0.014)	Loss 0.5285 (0.2889)	Prec@1 87.500 (89.734)	Prec@5 98.750 (99.754)
2018-12-06 15:57:56 - INFO - TRAINING - Epoch: [19][195/196]	Time 0.068 (0.196)	Data 0.000 (0.014)	Loss 0.3399 (0.2946)	Prec@1 86.250 (89.558)	Prec@5 100.000 (99.752)
2018-12-06 15:58:34 - INFO - TRAINING - Epoch: [20][195/196]	Time 0.039 (0.193)	Data 0.000 (0.014)	Loss 0.3847 (0.2908)	Prec@1 91.250 (89.918)	Prec@5 97.500 (99.720)
2018-12-06 15:58:42 - INFO - EVALUATING - Epoch: [20][39/40]	Time 0.061 (0.186)	Data 0.000 (0.071)	Loss 0.1914 (0.5409)	Prec@1 87.500 (83.480)	Prec@5 100.000 (99.130)
2018-12-06 15:59:19 - INFO - TRAINING - Epoch: [21][195/196]	Time 0.107 (0.192)	Data 0.000 (0.021)	Loss 0.1916 (0.2885)	Prec@1 93.750 (89.804)	Prec@5 100.000 (99.742)
2018-12-06 15:59:57 - INFO - TRAINING - Epoch: [22][195/196]	Time 0.389 (0.190)	Data 0.000 (0.021)	Loss 0.3093 (0.2836)	Prec@1 88.750 (89.916)	Prec@5 98.750 (99.778)
2018-12-06 16:00:34 - INFO - TRAINING - Epoch: [23][195/196]	Time 0.039 (0.189)	Data 0.000 (0.025)	Loss 0.2698 (0.2875)	Prec@1 90.000 (90.018)	Prec@5 100.000 (99.754)
2018-12-06 16:01:13 - INFO - TRAINING - Epoch: [24][195/196]	Time 0.053 (0.197)	Data 0.000 (0.029)	Loss 0.2669 (0.2869)	Prec@1 91.250 (89.986)	Prec@5 100.000 (99.798)
2018-12-06 16:01:51 - INFO - TRAINING - Epoch: [25][195/196]	Time 0.039 (0.193)	Data 0.000 (0.028)	Loss 0.2649 (0.2900)	Prec@1 88.750 (89.784)	Prec@5 100.000 (99.738)
2018-12-06 16:02:31 - INFO - TRAINING - Epoch: [26][195/196]	Time 0.088 (0.201)	Data 0.000 (0.020)	Loss 0.2590 (0.2873)	Prec@1 93.750 (89.890)	Prec@5 100.000 (99.764)
2018-12-06 16:03:09 - INFO - TRAINING - Epoch: [27][195/196]	Time 0.103 (0.192)	Data 0.000 (0.014)	Loss 0.3254 (0.2903)	Prec@1 88.750 (89.638)	Prec@5 100.000 (99.772)
2018-12-06 16:03:47 - INFO - TRAINING - Epoch: [28][195/196]	Time 0.098 (0.195)	Data 0.000 (0.014)	Loss 0.1614 (0.2835)	Prec@1 93.750 (89.992)	Prec@5 100.000 (99.784)
2018-12-06 16:04:25 - INFO - TRAINING - Epoch: [29][195/196]	Time 0.059 (0.193)	Data 0.000 (0.016)	Loss 0.2286 (0.2847)	Prec@1 92.500 (90.012)	Prec@5 100.000 (99.784)
2018-12-06 16:05:04 - INFO - TRAINING - Epoch: [30][195/196]	Time 0.110 (0.195)	Data 0.000 (0.016)	Loss 0.2304 (0.2805)	Prec@1 91.250 (90.132)	Prec@5 100.000 (99.774)
2018-12-06 16:05:42 - INFO - TRAINING - Epoch: [31][195/196]	Time 0.092 (0.192)	Data 0.000 (0.017)	Loss 0.3192 (0.2807)	Prec@1 91.250 (89.968)	Prec@5 100.000 (99.772)
2018-12-06 16:06:20 - INFO - TRAINING - Epoch: [32][195/196]	Time 0.099 (0.193)	Data 0.000 (0.018)	Loss 0.3525 (0.2829)	Prec@1 88.750 (90.066)	Prec@5 100.000 (99.756)
2018-12-06 16:06:58 - INFO - TRAINING - Epoch: [33][195/196]	Time 0.088 (0.195)	Data 0.000 (0.019)	Loss 0.4157 (0.2858)	Prec@1 87.500 (89.798)	Prec@5 98.750 (99.772)
2018-12-06 16:07:36 - INFO - TRAINING - Epoch: [34][195/196]	Time 0.093 (0.193)	Data 0.000 (0.020)	Loss 0.3901 (0.2783)	Prec@1 85.000 (90.220)	Prec@5 100.000 (99.788)
2018-12-06 16:08:14 - INFO - TRAINING - Epoch: [35][195/196]	Time 0.090 (0.194)	Data 0.000 (0.021)	Loss 0.2487 (0.2767)	Prec@1 90.000 (90.316)	Prec@5 100.000 (99.762)
2018-12-06 16:08:53 - INFO - TRAINING - Epoch: [36][195/196]	Time 0.081 (0.196)	Data 0.000 (0.021)	Loss 0.4237 (0.2778)	Prec@1 85.000 (90.248)	Prec@5 98.750 (99.778)
2018-12-06 16:09:31 - INFO - TRAINING - Epoch: [37][195/196]	Time 0.092 (0.193)	Data 0.000 (0.021)	Loss 0.2030 (0.2746)	Prec@1 92.500 (90.300)	Prec@5 100.000 (99.784)
2018-12-06 16:10:09 - INFO - TRAINING - Epoch: [38][195/196]	Time 0.101 (0.193)	Data 0.000 (0.021)	Loss 0.3731 (0.2746)	Prec@1 87.500 (90.422)	Prec@5 98.750 (99.788)
2018-12-06 16:10:47 - INFO - TRAINING - Epoch: [39][195/196]	Time 0.094 (0.195)	Data 0.000 (0.021)	Loss 0.2251 (0.2762)	Prec@1 91.250 (90.336)	Prec@5 100.000 (99.776)
2018-12-06 16:11:25 - INFO - TRAINING - Epoch: [40][195/196]	Time 0.115 (0.193)	Data 0.000 (0.021)	Loss 0.4556 (0.2750)	Prec@1 82.500 (90.250)	Prec@5 100.000 (99.780)
2018-12-06 16:11:33 - INFO - EVALUATING - Epoch: [40][39/40]	Time 0.074 (0.193)	Data 0.000 (0.101)	Loss 0.5304 (0.5597)	Prec@1 81.250 (82.910)	Prec@5 100.000 (98.960)
2018-12-06 16:12:11 - INFO - TRAINING - Epoch: [41][195/196]	Time 0.098 (0.193)	Data 0.000 (0.013)	Loss 0.2996 (0.2809)	Prec@1 86.250 (90.240)	Prec@5 98.750 (99.756)
2018-12-06 16:12:50 - INFO - TRAINING - Epoch: [42][195/196]	Time 0.094 (0.195)	Data 0.000 (0.014)	Loss 0.2221 (0.2798)	Prec@1 91.250 (90.038)	Prec@5 100.000 (99.772)
2018-12-06 16:13:28 - INFO - TRAINING - Epoch: [43][195/196]	Time 0.096 (0.193)	Data 0.000 (0.015)	Loss 0.2985 (0.2728)	Prec@1 90.000 (90.354)	Prec@5 100.000 (99.786)
2018-12-06 16:14:07 - INFO - TRAINING - Epoch: [44][195/196]	Time 0.096 (0.197)	Data 0.000 (0.017)	Loss 0.2308 (0.2695)	Prec@1 91.250 (90.524)	Prec@5 100.000 (99.792)
2018-12-06 16:14:45 - INFO - TRAINING - Epoch: [45][195/196]	Time 0.091 (0.193)	Data 0.000 (0.017)	Loss 0.2928 (0.2714)	Prec@1 90.000 (90.428)	Prec@5 100.000 (99.840)
2018-12-06 16:15:23 - INFO - TRAINING - Epoch: [46][195/196]	Time 0.098 (0.192)	Data 0.000 (0.018)	Loss 0.3420 (0.2780)	Prec@1 88.750 (89.972)	Prec@5 100.000 (99.764)
2018-12-06 16:16:03 - INFO - TRAINING - Epoch: [47][195/196]	Time 0.117 (0.203)	Data 0.000 (0.023)	Loss 0.2846 (0.2727)	Prec@1 91.250 (90.398)	Prec@5 98.750 (99.794)
2018-12-06 16:16:41 - INFO - TRAINING - Epoch: [48][195/196]	Time 0.071 (0.195)	Data 0.000 (0.022)	Loss 0.3095 (0.2769)	Prec@1 88.750 (90.280)	Prec@5 98.750 (99.776)
2018-12-06 16:17:20 - INFO - TRAINING - Epoch: [49][195/196]	Time 0.066 (0.195)	Data 0.000 (0.021)	Loss 0.4576 (0.2712)	Prec@1 80.000 (90.474)	Prec@5 98.750 (99.780)
2018-12-06 16:17:58 - INFO - TRAINING - Epoch: [50][195/196]	Time 0.102 (0.194)	Data 0.000 (0.021)	Loss 0.2208 (0.2752)	Prec@1 92.500 (90.362)	Prec@5 100.000 (99.790)
2018-12-06 16:18:36 - INFO - TRAINING - Epoch: [51][195/196]	Time 0.099 (0.192)	Data 0.000 (0.021)	Loss 0.2805 (0.2758)	Prec@1 91.250 (90.306)	Prec@5 100.000 (99.782)
2018-12-06 16:19:14 - INFO - TRAINING - Epoch: [52][195/196]	Time 0.105 (0.192)	Data 0.000 (0.021)	Loss 0.2978 (0.2706)	Prec@1 91.250 (90.370)	Prec@5 100.000 (99.794)
2018-12-06 16:19:52 - INFO - TRAINING - Epoch: [53][195/196]	Time 0.102 (0.194)	Data 0.000 (0.021)	Loss 0.2821 (0.2731)	Prec@1 92.500 (90.496)	Prec@5 100.000 (99.774)
2018-12-06 16:20:30 - INFO - TRAINING - Epoch: [54][195/196]	Time 0.101 (0.195)	Data 0.000 (0.021)	Loss 0.2297 (0.2724)	Prec@1 90.000 (90.448)	Prec@5 100.000 (99.796)
2018-12-06 16:21:10 - INFO - TRAINING - Epoch: [55][195/196]	Time 0.095 (0.201)	Data 0.000 (0.021)	Loss 0.3312 (0.2675)	Prec@1 91.250 (90.710)	Prec@5 100.000 (99.806)
2018-12-06 16:21:50 - INFO - TRAINING - Epoch: [56][195/196]	Time 0.113 (0.201)	Data 0.000 (0.022)	Loss 0.3632 (0.2747)	Prec@1 88.750 (90.316)	Prec@5 98.750 (99.786)
2018-12-06 16:22:30 - INFO - TRAINING - Epoch: [57][195/196]	Time 0.105 (0.203)	Data 0.000 (0.022)	Loss 0.2291 (0.2730)	Prec@1 88.750 (90.282)	Prec@5 100.000 (99.768)
2018-12-06 16:23:08 - INFO - TRAINING - Epoch: [58][195/196]	Time 0.095 (0.196)	Data 0.000 (0.022)	Loss 0.2511 (0.2664)	Prec@1 93.750 (90.606)	Prec@5 100.000 (99.806)
2018-12-06 16:23:46 - INFO - TRAINING - Epoch: [59][195/196]	Time 0.061 (0.193)	Data 0.000 (0.021)	Loss 0.3611 (0.2713)	Prec@1 88.750 (90.480)	Prec@5 100.000 (99.780)
2018-12-06 16:24:25 - INFO - TRAINING - Epoch: [60][195/196]	Time 0.060 (0.195)	Data 0.000 (0.021)	Loss 0.2464 (0.2694)	Prec@1 90.000 (90.560)	Prec@5 100.000 (99.808)
2018-12-06 16:24:33 - INFO - EVALUATING - Epoch: [60][39/40]	Time 0.027 (0.192)	Data 0.000 (0.102)	Loss 0.2790 (0.5028)	Prec@1 87.500 (83.850)	Prec@5 100.000 (99.510)
